{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Qassida.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Qassida : un générateur de poésie arabe avec LSTM sous Pytorch\n",
        "\n",
        "\n",
        "*   A rédiger : Membres de l'équipe\n",
        "*   Introduction + problématique \n",
        "*   Etapes de la réalistion\n",
        "\n"
      ],
      "metadata": {
        "id": "2zn4yMbbTFMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import des librairies :     \n",
        "Nous commençons par l'import des librairies dont nous aurons besoin pour pouvoir implémenter notre solution\n",
        "> * NumPy pour la gestion des calculs matriciels et vectoriels\n",
        "> * PyTorch permet d'effectuer les calculs tensoriels nécessaires notamment pour l'apprentissage profond\n",
        "> * Beautiful Soup pour le web scraping et la génération du dataset  "
      ],
      "metadata": {
        "id": "TkNPr3NnTx2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as func\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bBkbe9RYGtn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chargement des données\n",
        "Initialement nous exécutons un programme de web scraping qui fournit les données sous forme d'un fichier texte, pour effectuer l'entrainement après.\n",
        "\n",
        "> Nous chargeons d'abord, le résultat du scraping à partir du fichier texte pour le pré-traiter "
      ],
      "metadata": {
        "id": "Ei-bJa8dHABh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ouvrir le fichier texte lire les données sous forme de 'texte'\n",
        "\n",
        "with open('poems_of_Darwish.txt', 'r', encoding=\"utf-8\") as f:\n",
        "  poem = f.read()"
      ],
      "metadata": {
        "id": "eJI4gzVmIkyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Visualisons les 50 premiers caractères du texte :"
      ],
      "metadata": {
        "id": "kK3e5FMKJ21b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tdTIH7BdJ9WB",
        "outputId": "1b2f4cf0-70a4-49b7-dd9e-0d48c7b83dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'علي شاطء البحر بنت  وللبنت اهل\\nوللاهل بيت  وللبيت نافذتان وباب\\nوفي البحر بارجه تتسلي\\nبصيد المشاه علي'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorisation\n",
        "Dans ce qui suit, nous utilisons les structures 'dictionaries' de python pour convertir les caractères en un entier unique"
      ],
      "metadata": {
        "id": "GY1XQu7FLyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction 1 : char_to_int \n",
        "# Fonction 2 : int_to_char \n",
        "\n",
        "chars = tuple(set(poem))\n",
        "int_to_char = dict(enumerate(chars))\n",
        "char_to_int = {ch: ii for ii, ch in int_to_char.items()}\n",
        "\n",
        "encoded = np.array([char_to_int[ch] for ch in poem])\n"
      ],
      "metadata": {
        "id": "ad3cT7kHLtjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Visualisons l'encodage des 50 premiers caractères"
      ],
      "metadata": {
        "id": "1Y_pYyRbNp-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecUx5Y0HNy-3",
        "outputId": "cacdd627-f5c6-4d7e-f315-1d795e856d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48, 33, 44, 55, 60, 53, 64, 39, 55, 53, 33, 41,  2, 15, 55, 41, 25,\n",
              "       14, 55, 55, 16, 33, 33, 41, 25, 14, 55, 53, 26, 33, 11, 16, 33, 33,\n",
              "       53, 26, 33, 55, 41, 44, 14, 55, 55, 16, 33, 33, 41, 44, 14, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partie pré-traitement des données\n",
        "\n",
        "Dans le modèle charRNN que nous voulant utiliser, le LSTM prend en entrée des données one-hot-encoded \n",
        "i.e un vecteur dont la taille est celle du vocabulaire et pour chaque caractère un '1' est placé à\n",
        "la position du caractère dans ce vecteur, sinon '0'"
      ],
      "metadata": {
        "id": "_aM8wwlTQGDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "7vPwg8mjN2Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Division en mini-batches d'entrainement\n",
        "\n",
        "> On divise le vecteur des caractères encodés en séquence de taile seq_length selon le batch_size.\n"
      ],
      "metadata": {
        "id": "anqZrtAoQluw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Création des batches\n",
        "> 1 - D'abord, il faut retrancher certain caractères pour avoir des batches de taille complète uniquement.\n",
        "> 2 - Diviser le vecteur de caractères en N batches.\n",
        "> 3 - Parcourir le vecteur qui est maintenant divisé en N sous-vecteurs pour avoir les mini-batches. "
      ],
      "metadata": {
        "id": "0jZG5sUgQPIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrainer et tester les Batched\n",
        "> 1 - Créer 2 batches x,y où : x est le input batch et y est le training batch qui est exactement x mais décalé d'un seul caractère."
      ],
      "metadata": {
        "id": "GDWlelXnRuvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "\n",
        "    batch_size_total = batch_size * seq_length\n",
        "\n",
        "    # Nombre total de batches que nous pouvons avoir\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Garder que les batches complets \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "\n",
        "    # Reshape en lignes de batch_size \n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # Itérer sur chaque séquence de caractères\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        \n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "sh9y2OhSSByl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualiser la sortie \n",
        "> Visualisons ce que les batches sur 100 caractères de données encodées donne :"
      ],
      "metadata": {
        "id": "-4KCO0DoTAg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "# Les 10 premiers éléments d'une séquence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdynGBa2Slw_",
        "outputId": "3ee7f44e-aef4-4f56-ef3e-fc7137c841ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[48 33 44 55 60 53 64 39 55 53]\n",
            " [53 55 53 33  4 53 26 15 44 25]\n",
            " [12 55 53 33 16 24 14 55  3 44]\n",
            " [11 16  4 15 25 53 55 53 33 44]\n",
            " [55 40 25 55 53 33 15 16  2 55]\n",
            " [16 55 33 61 26 11 53 33 40 55]\n",
            " [53 55 40 53 39 26 53 55 44 15]\n",
            " [25 16 53 48 55 53 33  2 57 53]]\n",
            "\n",
            "y\n",
            " [[33 44 55 60 53 64 39 55 53 33]\n",
            " [55 53 33  4 53 26 15 44 25 55]\n",
            " [55 53 33 16 24 14 55  3 44 55]\n",
            " [16  4 15 25 53 55 53 33 44 55]\n",
            " [40 25 55 53 33 15 16  2 55 16]\n",
            " [55 33 61 26 11 53 33 40 55 14]\n",
            " [55 40 53 39 26 53 55 44 15 16]\n",
            " [16 53 48 55 53 33  2 57 53 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Définir le modèle avec PyTorch\n",
        "> On commence d'abord par définir les couches (layers) du modèle ensuite les fonctions (opérations), après on doit aussi écrire le code pour les opérations : forward (passer un caractère)\n",
        "\n",
        "\n",
        "> __Remarque__ : on essaye d'utiliser les GPUs disponibles sinon, sélectionner le CPU"
      ],
      "metadata": {
        "id": "bpuwSRq3cg5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('L\\'entrainement se fera sur GPU')\n",
        "else: \n",
        "    print('Aucun GPU trouvé ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8jbTDrzbM4L",
        "outputId": "345775e4-4f2f-48fa-8f43-0f5623ee9ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L'entrainement se fera sur GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Dictionnaires des caractères \n",
        "        self.chars = tokens\n",
        "        self.int_to_char = dict(enumerate(self.chars))\n",
        "        self.char_to_int = {ch: ii for ii, ch in self.int_to_char.items()}\n",
        "        \n",
        "        ## Définir le LSTM \n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## Définir la couche de dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## Définir la couche de sortie\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                \n",
        "        ## Recevoir la sortie et le nouvel état 'hidden' depuis le LSTM\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## Passer par une couche de dropout\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Empiler (stacking) les sorties des LSTM \n",
        "        # Ici contiguous() c'est pour faire le reshape du vecteur \n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## Faire passer x dans la couche entièrement connecté\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # Retourner la sortie finale et le hidden state final\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initialiser le hidden state '''\n",
        "        # Créations de 2 tensors de dimensions n_layers x batch_size x n_hidden,\n",
        "        # initialiser à 0 pour le hidden state et cell state (cellule état) du LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "metadata": {
        "id": "ZEzg97bIdR5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs=5, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Entrainer le modèle\n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: Le réseau charRNN\n",
        "        data: Texte du dataset \n",
        "        epochs: Nombre d'epochs pour entrainer \n",
        "        batch_size: Nombre de mini-séquences par mini-batch (taille du batch)\n",
        "\n",
        "        seq_length: Nombre de caractères par mini-batch\n",
        "        lr: learning rate (taux d'apprentissage)\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction de données à garder pour l'étape de validation\n",
        "        print_every: Nombre d'étapes à compter jusqu'au prochain affichage du loss\n",
        "    \n",
        "    '''\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    valid_loss_min = np.Inf \n",
        "\n",
        "\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Création des données de less et de validation\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialiser l'état hidden \n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            x = one_hot(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # initialiser les gradients cumulés \n",
        "            net.zero_grad()\n",
        "            \n",
        "            # Recevoir la sortie \n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # Calculer la perte (loss) et effectuer une backpropagation\n",
        "            loss = train_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` permet de controler l'explosion du gradient dans RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # statistiques du loss\n",
        "            if counter % print_every == 0:\n",
        "                \n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    \n",
        "                    x = one_hot(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    \n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = valid_loss =  criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # Remettre en état d'entrainement après la phase de validation\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Etape: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                "
      ],
      "metadata": {
        "id": "fhO6CTPVergM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrainement\n",
        "> __Remarque :__ L'entrainement sur Google Colab avec 50 epoch prend 4 heurs."
      ],
      "metadata": {
        "id": "N7InzVHSjkNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir le network (réseau)\n",
        "n_hidden=128\n",
        "n_layers=3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMixFNI2jieh",
        "outputId": "bbd1c7e9-c832-460d-c265-d639a7936da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(67, 128, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=67, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seq_length = 100\n",
        "n_epochs = 100\n",
        "\n",
        "# Entrainer le modèle \n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "metadata": {
        "id": "coiKo_wNj9Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049c0b8a-439e-45ff-fe3b-6f32b629041b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100... Etape: 10... Loss: 3.7269... Val Loss: 3.4927\n",
            "Epoch: 1/100... Etape: 20... Loss: 3.1132... Val Loss: 3.0338\n",
            "Epoch: 1/100... Etape: 30... Loss: 3.0434... Val Loss: 2.9763\n",
            "Epoch: 1/100... Etape: 40... Loss: 3.0005... Val Loss: 2.9696\n",
            "Epoch: 1/100... Etape: 50... Loss: 3.0019... Val Loss: 2.9648\n",
            "Epoch: 1/100... Etape: 60... Loss: 2.9905... Val Loss: 2.9633\n",
            "Epoch: 1/100... Etape: 70... Loss: 2.9908... Val Loss: 2.9623\n",
            "Epoch: 1/100... Etape: 80... Loss: 2.9636... Val Loss: 2.9612\n",
            "Epoch: 1/100... Etape: 90... Loss: 2.9928... Val Loss: 2.9621\n",
            "Epoch: 1/100... Etape: 100... Loss: 3.0104... Val Loss: 2.9595\n",
            "Epoch: 1/100... Etape: 110... Loss: 2.9801... Val Loss: 2.9594\n",
            "Epoch: 1/100... Etape: 120... Loss: 2.9414... Val Loss: 2.9607\n",
            "Epoch: 1/100... Etape: 130... Loss: 2.9675... Val Loss: 2.9582\n",
            "Epoch: 1/100... Etape: 140... Loss: 2.9593... Val Loss: 2.9587\n",
            "Epoch: 1/100... Etape: 150... Loss: 2.9761... Val Loss: 2.9566\n",
            "Epoch: 1/100... Etape: 160... Loss: 2.9715... Val Loss: 2.9569\n",
            "Epoch: 1/100... Etape: 170... Loss: 2.9667... Val Loss: 2.9561\n",
            "Epoch: 1/100... Etape: 180... Loss: 2.9303... Val Loss: 2.9565\n",
            "Epoch: 2/100... Etape: 190... Loss: 2.9626... Val Loss: 2.9583\n",
            "Epoch: 2/100... Etape: 200... Loss: 2.9368... Val Loss: 2.9601\n",
            "Epoch: 2/100... Etape: 210... Loss: 2.9471... Val Loss: 2.9561\n",
            "Epoch: 2/100... Etape: 220... Loss: 2.9316... Val Loss: 2.9554\n",
            "Epoch: 2/100... Etape: 230... Loss: 2.9450... Val Loss: 2.9559\n",
            "Epoch: 2/100... Etape: 240... Loss: 2.9438... Val Loss: 2.9544\n",
            "Epoch: 2/100... Etape: 250... Loss: 2.9408... Val Loss: 2.9547\n",
            "Epoch: 2/100... Etape: 260... Loss: 2.9695... Val Loss: 2.9533\n",
            "Epoch: 2/100... Etape: 270... Loss: 2.9558... Val Loss: 2.9546\n",
            "Epoch: 2/100... Etape: 280... Loss: 2.9779... Val Loss: 2.9549\n",
            "Epoch: 2/100... Etape: 290... Loss: 2.9912... Val Loss: 2.9544\n",
            "Epoch: 2/100... Etape: 300... Loss: 2.9294... Val Loss: 2.9558\n",
            "Epoch: 2/100... Etape: 310... Loss: 2.9575... Val Loss: 2.9561\n",
            "Epoch: 2/100... Etape: 320... Loss: 2.9379... Val Loss: 2.9549\n",
            "Epoch: 2/100... Etape: 330... Loss: 2.9524... Val Loss: 2.9546\n",
            "Epoch: 2/100... Etape: 340... Loss: 2.9390... Val Loss: 2.9533\n",
            "Epoch: 2/100... Etape: 350... Loss: 2.9464... Val Loss: 2.9532\n",
            "Epoch: 2/100... Etape: 360... Loss: 2.9167... Val Loss: 2.9532\n",
            "Epoch: 2/100... Etape: 370... Loss: 2.9507... Val Loss: 2.9536\n",
            "Epoch: 3/100... Etape: 380... Loss: 2.9253... Val Loss: 2.9543\n",
            "Epoch: 3/100... Etape: 390... Loss: 2.9230... Val Loss: 2.9524\n",
            "Epoch: 3/100... Etape: 400... Loss: 2.9438... Val Loss: 2.9397\n",
            "Epoch: 3/100... Etape: 410... Loss: 2.9220... Val Loss: 2.9284\n",
            "Epoch: 3/100... Etape: 420... Loss: 2.8938... Val Loss: 2.8972\n",
            "Epoch: 3/100... Etape: 430... Loss: 2.8578... Val Loss: 2.8652\n",
            "Epoch: 3/100... Etape: 440... Loss: 2.8214... Val Loss: 2.8353\n",
            "Epoch: 3/100... Etape: 450... Loss: 2.8087... Val Loss: 2.8049\n",
            "Epoch: 3/100... Etape: 460... Loss: 2.7924... Val Loss: 2.7926\n",
            "Epoch: 3/100... Etape: 470... Loss: 2.7705... Val Loss: 2.7749\n",
            "Epoch: 3/100... Etape: 480... Loss: 2.7809... Val Loss: 2.7618\n",
            "Epoch: 3/100... Etape: 490... Loss: 2.7576... Val Loss: 2.7601\n",
            "Epoch: 3/100... Etape: 500... Loss: 2.7573... Val Loss: 2.7551\n",
            "Epoch: 3/100... Etape: 510... Loss: 2.7622... Val Loss: 2.7503\n",
            "Epoch: 3/100... Etape: 520... Loss: 2.7504... Val Loss: 2.7423\n",
            "Epoch: 3/100... Etape: 530... Loss: 2.7280... Val Loss: 2.7363\n",
            "Epoch: 3/100... Etape: 540... Loss: 2.7529... Val Loss: 2.7335\n",
            "Epoch: 3/100... Etape: 550... Loss: 2.7380... Val Loss: 2.7293\n",
            "Epoch: 4/100... Etape: 560... Loss: 2.7124... Val Loss: 2.7239\n",
            "Epoch: 4/100... Etape: 570... Loss: 2.7048... Val Loss: 2.7208\n",
            "Epoch: 4/100... Etape: 580... Loss: 2.6952... Val Loss: 2.7111\n",
            "Epoch: 4/100... Etape: 590... Loss: 2.7544... Val Loss: 2.7016\n",
            "Epoch: 4/100... Etape: 600... Loss: 2.6925... Val Loss: 2.6909\n",
            "Epoch: 4/100... Etape: 610... Loss: 2.6902... Val Loss: 2.6818\n",
            "Epoch: 4/100... Etape: 620... Loss: 2.6887... Val Loss: 2.6730\n",
            "Epoch: 4/100... Etape: 630... Loss: 2.6835... Val Loss: 2.6623\n",
            "Epoch: 4/100... Etape: 640... Loss: 2.6643... Val Loss: 2.6591\n",
            "Epoch: 4/100... Etape: 650... Loss: 2.6782... Val Loss: 2.6554\n",
            "Epoch: 4/100... Etape: 660... Loss: 2.6801... Val Loss: 2.6530\n",
            "Epoch: 4/100... Etape: 670... Loss: 2.6179... Val Loss: 2.6423\n",
            "Epoch: 4/100... Etape: 680... Loss: 2.6692... Val Loss: 2.6403\n",
            "Epoch: 4/100... Etape: 690... Loss: 2.6538... Val Loss: 2.6389\n",
            "Epoch: 4/100... Etape: 700... Loss: 2.6677... Val Loss: 2.6343\n",
            "Epoch: 4/100... Etape: 710... Loss: 2.6284... Val Loss: 2.6282\n",
            "Epoch: 4/100... Etape: 720... Loss: 2.6237... Val Loss: 2.6213\n",
            "Epoch: 4/100... Etape: 730... Loss: 2.6070... Val Loss: 2.6176\n",
            "Epoch: 4/100... Etape: 740... Loss: 2.6150... Val Loss: 2.6166\n",
            "Epoch: 5/100... Etape: 750... Loss: 2.6178... Val Loss: 2.6157\n",
            "Epoch: 5/100... Etape: 760... Loss: 2.5856... Val Loss: 2.6110\n",
            "Epoch: 5/100... Etape: 770... Loss: 2.6238... Val Loss: 2.6092\n",
            "Epoch: 5/100... Etape: 780... Loss: 2.6153... Val Loss: 2.6035\n",
            "Epoch: 5/100... Etape: 790... Loss: 2.5934... Val Loss: 2.6071\n",
            "Epoch: 5/100... Etape: 800... Loss: 2.5701... Val Loss: 2.5995\n",
            "Epoch: 5/100... Etape: 810... Loss: 2.6082... Val Loss: 2.5948\n",
            "Epoch: 5/100... Etape: 820... Loss: 2.6027... Val Loss: 2.5939\n",
            "Epoch: 5/100... Etape: 830... Loss: 2.5632... Val Loss: 2.5907\n",
            "Epoch: 5/100... Etape: 840... Loss: 2.6084... Val Loss: 2.5879\n",
            "Epoch: 5/100... Etape: 850... Loss: 2.5767... Val Loss: 2.5874\n",
            "Epoch: 5/100... Etape: 860... Loss: 2.6275... Val Loss: 2.5871\n",
            "Epoch: 5/100... Etape: 870... Loss: 2.6087... Val Loss: 2.5831\n",
            "Epoch: 5/100... Etape: 880... Loss: 2.5966... Val Loss: 2.5819\n",
            "Epoch: 5/100... Etape: 890... Loss: 2.5661... Val Loss: 2.5804\n",
            "Epoch: 5/100... Etape: 900... Loss: 2.5965... Val Loss: 2.5780\n",
            "Epoch: 5/100... Etape: 910... Loss: 2.5726... Val Loss: 2.5741\n",
            "Epoch: 5/100... Etape: 920... Loss: 2.5762... Val Loss: 2.5721\n",
            "Epoch: 5/100... Etape: 930... Loss: 2.5754... Val Loss: 2.5718\n",
            "Epoch: 6/100... Etape: 940... Loss: 2.5656... Val Loss: 2.5722\n",
            "Epoch: 6/100... Etape: 950... Loss: 2.5532... Val Loss: 2.5680\n",
            "Epoch: 6/100... Etape: 960... Loss: 2.5627... Val Loss: 2.5671\n",
            "Epoch: 6/100... Etape: 970... Loss: 2.5353... Val Loss: 2.5626\n",
            "Epoch: 6/100... Etape: 980... Loss: 2.5438... Val Loss: 2.5614\n",
            "Epoch: 6/100... Etape: 990... Loss: 2.5507... Val Loss: 2.5582\n",
            "Epoch: 6/100... Etape: 1000... Loss: 2.5998... Val Loss: 2.5540\n",
            "Epoch: 6/100... Etape: 1010... Loss: 2.5274... Val Loss: 2.5533\n",
            "Epoch: 6/100... Etape: 1020... Loss: 2.5768... Val Loss: 2.5528\n",
            "Epoch: 6/100... Etape: 1030... Loss: 2.5808... Val Loss: 2.5504\n",
            "Epoch: 6/100... Etape: 1040... Loss: 2.5695... Val Loss: 2.5506\n",
            "Epoch: 6/100... Etape: 1050... Loss: 2.5219... Val Loss: 2.5469\n",
            "Epoch: 6/100... Etape: 1060... Loss: 2.5791... Val Loss: 2.5455\n",
            "Epoch: 6/100... Etape: 1070... Loss: 2.5698... Val Loss: 2.5453\n",
            "Epoch: 6/100... Etape: 1080... Loss: 2.5553... Val Loss: 2.5412\n",
            "Epoch: 6/100... Etape: 1090... Loss: 2.5303... Val Loss: 2.5385\n",
            "Epoch: 6/100... Etape: 1100... Loss: 2.5612... Val Loss: 2.5386\n",
            "Epoch: 6/100... Etape: 1110... Loss: 2.5015... Val Loss: 2.5381\n",
            "Epoch: 7/100... Etape: 1120... Loss: 2.5406... Val Loss: 2.5371\n",
            "Epoch: 7/100... Etape: 1130... Loss: 2.5142... Val Loss: 2.5353\n",
            "Epoch: 7/100... Etape: 1140... Loss: 2.5161... Val Loss: 2.5335\n",
            "Epoch: 7/100... Etape: 1150... Loss: 2.5461... Val Loss: 2.5355\n",
            "Epoch: 7/100... Etape: 1160... Loss: 2.5249... Val Loss: 2.5288\n",
            "Epoch: 7/100... Etape: 1170... Loss: 2.5225... Val Loss: 2.5266\n",
            "Epoch: 7/100... Etape: 1180... Loss: 2.5388... Val Loss: 2.5241\n",
            "Epoch: 7/100... Etape: 1190... Loss: 2.5822... Val Loss: 2.5235\n",
            "Epoch: 7/100... Etape: 1200... Loss: 2.5519... Val Loss: 2.5202\n",
            "Epoch: 7/100... Etape: 1210... Loss: 2.5927... Val Loss: 2.5223\n",
            "Epoch: 7/100... Etape: 1220... Loss: 2.5566... Val Loss: 2.5190\n",
            "Epoch: 7/100... Etape: 1230... Loss: 2.5197... Val Loss: 2.5183\n",
            "Epoch: 7/100... Etape: 1240... Loss: 2.5648... Val Loss: 2.5199\n",
            "Epoch: 7/100... Etape: 1250... Loss: 2.5175... Val Loss: 2.5177\n",
            "Epoch: 7/100... Etape: 1260... Loss: 2.5582... Val Loss: 2.5160\n",
            "Epoch: 7/100... Etape: 1270... Loss: 2.5002... Val Loss: 2.5134\n",
            "Epoch: 7/100... Etape: 1280... Loss: 2.4821... Val Loss: 2.5088\n",
            "Epoch: 7/100... Etape: 1290... Loss: 2.4788... Val Loss: 2.5096\n",
            "Epoch: 7/100... Etape: 1300... Loss: 2.5341... Val Loss: 2.5084\n",
            "Epoch: 8/100... Etape: 1310... Loss: 2.4936... Val Loss: 2.5080\n",
            "Epoch: 8/100... Etape: 1320... Loss: 2.4738... Val Loss: 2.5054\n",
            "Epoch: 8/100... Etape: 1330... Loss: 2.5384... Val Loss: 2.5047\n",
            "Epoch: 8/100... Etape: 1340... Loss: 2.4947... Val Loss: 2.5036\n",
            "Epoch: 8/100... Etape: 1350... Loss: 2.5109... Val Loss: 2.4990\n",
            "Epoch: 8/100... Etape: 1360... Loss: 2.4898... Val Loss: 2.4988\n",
            "Epoch: 8/100... Etape: 1370... Loss: 2.4547... Val Loss: 2.4954\n",
            "Epoch: 8/100... Etape: 1380... Loss: 2.5116... Val Loss: 2.4946\n",
            "Epoch: 8/100... Etape: 1390... Loss: 2.4832... Val Loss: 2.4953\n",
            "Epoch: 8/100... Etape: 1400... Loss: 2.4806... Val Loss: 2.4929\n",
            "Epoch: 8/100... Etape: 1410... Loss: 2.5491... Val Loss: 2.4925\n",
            "Epoch: 8/100... Etape: 1420... Loss: 2.5158... Val Loss: 2.4928\n",
            "Epoch: 8/100... Etape: 1430... Loss: 2.4994... Val Loss: 2.4896\n",
            "Epoch: 8/100... Etape: 1440... Loss: 2.5188... Val Loss: 2.4907\n",
            "Epoch: 8/100... Etape: 1450... Loss: 2.5150... Val Loss: 2.4867\n",
            "Epoch: 8/100... Etape: 1460... Loss: 2.5028... Val Loss: 2.4873\n",
            "Epoch: 8/100... Etape: 1470... Loss: 2.4926... Val Loss: 2.4827\n",
            "Epoch: 8/100... Etape: 1480... Loss: 2.5152... Val Loss: 2.4818\n",
            "Epoch: 9/100... Etape: 1490... Loss: 2.4979... Val Loss: 2.4826\n",
            "Epoch: 9/100... Etape: 1500... Loss: 2.4809... Val Loss: 2.4849\n",
            "Epoch: 9/100... Etape: 1510... Loss: 2.4593... Val Loss: 2.4809\n",
            "Epoch: 9/100... Etape: 1520... Loss: 2.5483... Val Loss: 2.4813\n",
            "Epoch: 9/100... Etape: 1530... Loss: 2.4873... Val Loss: 2.4779\n",
            "Epoch: 9/100... Etape: 1540... Loss: 2.5055... Val Loss: 2.4760\n",
            "Epoch: 9/100... Etape: 1550... Loss: 2.4962... Val Loss: 2.4747\n",
            "Epoch: 9/100... Etape: 1560... Loss: 2.4895... Val Loss: 2.4711\n",
            "Epoch: 9/100... Etape: 1570... Loss: 2.4781... Val Loss: 2.4692\n",
            "Epoch: 9/100... Etape: 1580... Loss: 2.4995... Val Loss: 2.4724\n",
            "Epoch: 9/100... Etape: 1590... Loss: 2.5135... Val Loss: 2.4723\n",
            "Epoch: 9/100... Etape: 1600... Loss: 2.4551... Val Loss: 2.4716\n",
            "Epoch: 9/100... Etape: 1610... Loss: 2.4993... Val Loss: 2.4677\n",
            "Epoch: 9/100... Etape: 1620... Loss: 2.4985... Val Loss: 2.4678\n",
            "Epoch: 9/100... Etape: 1630... Loss: 2.5091... Val Loss: 2.4668\n",
            "Epoch: 9/100... Etape: 1640... Loss: 2.4771... Val Loss: 2.4649\n",
            "Epoch: 9/100... Etape: 1650... Loss: 2.4632... Val Loss: 2.4614\n",
            "Epoch: 9/100... Etape: 1660... Loss: 2.4589... Val Loss: 2.4636\n",
            "Epoch: 9/100... Etape: 1670... Loss: 2.4572... Val Loss: 2.4621\n",
            "Epoch: 10/100... Etape: 1680... Loss: 2.4615... Val Loss: 2.4625\n",
            "Epoch: 10/100... Etape: 1690... Loss: 2.4362... Val Loss: 2.4610\n",
            "Epoch: 10/100... Etape: 1700... Loss: 2.5006... Val Loss: 2.4615\n",
            "Epoch: 10/100... Etape: 1710... Loss: 2.4657... Val Loss: 2.4602\n",
            "Epoch: 10/100... Etape: 1720... Loss: 2.4445... Val Loss: 2.4567\n",
            "Epoch: 10/100... Etape: 1730... Loss: 2.4189... Val Loss: 2.4565\n",
            "Epoch: 10/100... Etape: 1740... Loss: 2.4571... Val Loss: 2.4557\n",
            "Epoch: 10/100... Etape: 1750... Loss: 2.4638... Val Loss: 2.4513\n",
            "Epoch: 10/100... Etape: 1760... Loss: 2.4248... Val Loss: 2.4519\n",
            "Epoch: 10/100... Etape: 1770... Loss: 2.4807... Val Loss: 2.4520\n",
            "Epoch: 10/100... Etape: 1780... Loss: 2.4655... Val Loss: 2.4515\n",
            "Epoch: 10/100... Etape: 1790... Loss: 2.4720... Val Loss: 2.4517\n",
            "Epoch: 10/100... Etape: 1800... Loss: 2.4605... Val Loss: 2.4483\n",
            "Epoch: 10/100... Etape: 1810... Loss: 2.4679... Val Loss: 2.4489\n",
            "Epoch: 10/100... Etape: 1820... Loss: 2.4469... Val Loss: 2.4479\n",
            "Epoch: 10/100... Etape: 1830... Loss: 2.4771... Val Loss: 2.4459\n",
            "Epoch: 10/100... Etape: 1840... Loss: 2.4422... Val Loss: 2.4443\n",
            "Epoch: 10/100... Etape: 1850... Loss: 2.4588... Val Loss: 2.4449\n",
            "Epoch: 10/100... Etape: 1860... Loss: 2.4583... Val Loss: 2.4459\n",
            "Epoch: 11/100... Etape: 1870... Loss: 2.4721... Val Loss: 2.4460\n",
            "Epoch: 11/100... Etape: 1880... Loss: 2.4233... Val Loss: 2.4419\n",
            "Epoch: 11/100... Etape: 1890... Loss: 2.4555... Val Loss: 2.4450\n",
            "Epoch: 11/100... Etape: 1900... Loss: 2.4217... Val Loss: 2.4394\n",
            "Epoch: 11/100... Etape: 1910... Loss: 2.4278... Val Loss: 2.4396\n",
            "Epoch: 11/100... Etape: 1920... Loss: 2.4226... Val Loss: 2.4395\n",
            "Epoch: 11/100... Etape: 1930... Loss: 2.4796... Val Loss: 2.4367\n",
            "Epoch: 11/100... Etape: 1940... Loss: 2.4026... Val Loss: 2.4342\n",
            "Epoch: 11/100... Etape: 1950... Loss: 2.4686... Val Loss: 2.4380\n",
            "Epoch: 11/100... Etape: 1960... Loss: 2.4777... Val Loss: 2.4364\n",
            "Epoch: 11/100... Etape: 1970... Loss: 2.4537... Val Loss: 2.4355\n",
            "Epoch: 11/100... Etape: 1980... Loss: 2.4115... Val Loss: 2.4321\n",
            "Epoch: 11/100... Etape: 1990... Loss: 2.4689... Val Loss: 2.4311\n",
            "Epoch: 11/100... Etape: 2000... Loss: 2.4702... Val Loss: 2.4322\n",
            "Epoch: 11/100... Etape: 2010... Loss: 2.4461... Val Loss: 2.4299\n",
            "Epoch: 11/100... Etape: 2020... Loss: 2.4355... Val Loss: 2.4281\n",
            "Epoch: 11/100... Etape: 2030... Loss: 2.4639... Val Loss: 2.4278\n",
            "Epoch: 11/100... Etape: 2040... Loss: 2.4012... Val Loss: 2.4281\n",
            "Epoch: 12/100... Etape: 2050... Loss: 2.4310... Val Loss: 2.4294\n",
            "Epoch: 12/100... Etape: 2060... Loss: 2.4157... Val Loss: 2.4275\n",
            "Epoch: 12/100... Etape: 2070... Loss: 2.3922... Val Loss: 2.4265\n",
            "Epoch: 12/100... Etape: 2080... Loss: 2.4472... Val Loss: 2.4265\n",
            "Epoch: 12/100... Etape: 2090... Loss: 2.4262... Val Loss: 2.4244\n",
            "Epoch: 12/100... Etape: 2100... Loss: 2.4261... Val Loss: 2.4248\n",
            "Epoch: 12/100... Etape: 2110... Loss: 2.4248... Val Loss: 2.4255\n",
            "Epoch: 12/100... Etape: 2120... Loss: 2.4592... Val Loss: 2.4194\n",
            "Epoch: 12/100... Etape: 2130... Loss: 2.4532... Val Loss: 2.4196\n",
            "Epoch: 12/100... Etape: 2140... Loss: 2.4917... Val Loss: 2.4206\n",
            "Epoch: 12/100... Etape: 2150... Loss: 2.4490... Val Loss: 2.4191\n",
            "Epoch: 12/100... Etape: 2160... Loss: 2.4297... Val Loss: 2.4202\n",
            "Epoch: 12/100... Etape: 2170... Loss: 2.4501... Val Loss: 2.4186\n",
            "Epoch: 12/100... Etape: 2180... Loss: 2.4226... Val Loss: 2.4175\n",
            "Epoch: 12/100... Etape: 2190... Loss: 2.4590... Val Loss: 2.4160\n",
            "Epoch: 12/100... Etape: 2200... Loss: 2.4156... Val Loss: 2.4144\n",
            "Epoch: 12/100... Etape: 2210... Loss: 2.3803... Val Loss: 2.4124\n",
            "Epoch: 12/100... Etape: 2220... Loss: 2.3834... Val Loss: 2.4127\n",
            "Epoch: 12/100... Etape: 2230... Loss: 2.4472... Val Loss: 2.4137\n",
            "Epoch: 13/100... Etape: 2240... Loss: 2.4021... Val Loss: 2.4141\n",
            "Epoch: 13/100... Etape: 2250... Loss: 2.3957... Val Loss: 2.4116\n",
            "Epoch: 13/100... Etape: 2260... Loss: 2.4491... Val Loss: 2.4123\n",
            "Epoch: 13/100... Etape: 2270... Loss: 2.4032... Val Loss: 2.4088\n",
            "Epoch: 13/100... Etape: 2280... Loss: 2.4158... Val Loss: 2.4087\n",
            "Epoch: 13/100... Etape: 2290... Loss: 2.4092... Val Loss: 2.4087\n",
            "Epoch: 13/100... Etape: 2300... Loss: 2.3822... Val Loss: 2.4068\n",
            "Epoch: 13/100... Etape: 2310... Loss: 2.4199... Val Loss: 2.4036\n",
            "Epoch: 13/100... Etape: 2320... Loss: 2.3942... Val Loss: 2.4056\n",
            "Epoch: 13/100... Etape: 2330... Loss: 2.4119... Val Loss: 2.4059\n",
            "Epoch: 13/100... Etape: 2340... Loss: 2.4667... Val Loss: 2.4031\n",
            "Epoch: 13/100... Etape: 2350... Loss: 2.4488... Val Loss: 2.4012\n",
            "Epoch: 13/100... Etape: 2360... Loss: 2.4090... Val Loss: 2.4020\n",
            "Epoch: 13/100... Etape: 2370... Loss: 2.4197... Val Loss: 2.4026\n",
            "Epoch: 13/100... Etape: 2380... Loss: 2.4181... Val Loss: 2.3992\n",
            "Epoch: 13/100... Etape: 2390... Loss: 2.4082... Val Loss: 2.3982\n",
            "Epoch: 13/100... Etape: 2400... Loss: 2.4079... Val Loss: 2.3973\n",
            "Epoch: 13/100... Etape: 2410... Loss: 2.4254... Val Loss: 2.3984\n",
            "Epoch: 14/100... Etape: 2420... Loss: 2.4178... Val Loss: 2.4002\n",
            "Epoch: 14/100... Etape: 2430... Loss: 2.3983... Val Loss: 2.3986\n",
            "Epoch: 14/100... Etape: 2440... Loss: 2.3792... Val Loss: 2.3958\n",
            "Epoch: 14/100... Etape: 2450... Loss: 2.4647... Val Loss: 2.3971\n",
            "Epoch: 14/100... Etape: 2460... Loss: 2.4043... Val Loss: 2.3920\n",
            "Epoch: 14/100... Etape: 2470... Loss: 2.4206... Val Loss: 2.3938\n",
            "Epoch: 14/100... Etape: 2480... Loss: 2.4122... Val Loss: 2.3942\n",
            "Epoch: 14/100... Etape: 2490... Loss: 2.4143... Val Loss: 2.3910\n",
            "Epoch: 14/100... Etape: 2500... Loss: 2.3790... Val Loss: 2.3883\n",
            "Epoch: 14/100... Etape: 2510... Loss: 2.4354... Val Loss: 2.3913\n",
            "Epoch: 14/100... Etape: 2520... Loss: 2.4456... Val Loss: 2.3906\n",
            "Epoch: 14/100... Etape: 2530... Loss: 2.3760... Val Loss: 2.3907\n",
            "Epoch: 14/100... Etape: 2540... Loss: 2.4197... Val Loss: 2.3879\n",
            "Epoch: 14/100... Etape: 2550... Loss: 2.4212... Val Loss: 2.3865\n",
            "Epoch: 14/100... Etape: 2560... Loss: 2.4405... Val Loss: 2.3865\n",
            "Epoch: 14/100... Etape: 2570... Loss: 2.4069... Val Loss: 2.3854\n",
            "Epoch: 14/100... Etape: 2580... Loss: 2.3912... Val Loss: 2.3822\n",
            "Epoch: 14/100... Etape: 2590... Loss: 2.3712... Val Loss: 2.3837\n",
            "Epoch: 14/100... Etape: 2600... Loss: 2.3905... Val Loss: 2.3847\n",
            "Epoch: 15/100... Etape: 2610... Loss: 2.4046... Val Loss: 2.3858\n",
            "Epoch: 15/100... Etape: 2620... Loss: 2.3671... Val Loss: 2.3828\n",
            "Epoch: 15/100... Etape: 2630... Loss: 2.4296... Val Loss: 2.3807\n",
            "Epoch: 15/100... Etape: 2640... Loss: 2.3908... Val Loss: 2.3801\n",
            "Epoch: 15/100... Etape: 2650... Loss: 2.3649... Val Loss: 2.3786\n",
            "Epoch: 15/100... Etape: 2660... Loss: 2.3528... Val Loss: 2.3803\n",
            "Epoch: 15/100... Etape: 2670... Loss: 2.3856... Val Loss: 2.3794\n",
            "Epoch: 15/100... Etape: 2680... Loss: 2.3953... Val Loss: 2.3753\n",
            "Epoch: 15/100... Etape: 2690... Loss: 2.3424... Val Loss: 2.3761\n",
            "Epoch: 15/100... Etape: 2700... Loss: 2.3935... Val Loss: 2.3762\n",
            "Epoch: 15/100... Etape: 2710... Loss: 2.3867... Val Loss: 2.3755\n",
            "Epoch: 15/100... Etape: 2720... Loss: 2.4003... Val Loss: 2.3752\n",
            "Epoch: 15/100... Etape: 2730... Loss: 2.3801... Val Loss: 2.3738\n",
            "Epoch: 15/100... Etape: 2740... Loss: 2.3982... Val Loss: 2.3730\n",
            "Epoch: 15/100... Etape: 2750... Loss: 2.3525... Val Loss: 2.3709\n",
            "Epoch: 15/100... Etape: 2760... Loss: 2.3989... Val Loss: 2.3694\n",
            "Epoch: 15/100... Etape: 2770... Loss: 2.3674... Val Loss: 2.3678\n",
            "Epoch: 15/100... Etape: 2780... Loss: 2.3833... Val Loss: 2.3679\n",
            "Epoch: 15/100... Etape: 2790... Loss: 2.3780... Val Loss: 2.3695\n",
            "Epoch: 16/100... Etape: 2800... Loss: 2.4104... Val Loss: 2.3727\n",
            "Epoch: 16/100... Etape: 2810... Loss: 2.3548... Val Loss: 2.3653\n",
            "Epoch: 16/100... Etape: 2820... Loss: 2.3790... Val Loss: 2.3677\n",
            "Epoch: 16/100... Etape: 2830... Loss: 2.3428... Val Loss: 2.3654\n",
            "Epoch: 16/100... Etape: 2840... Loss: 2.3571... Val Loss: 2.3648\n",
            "Epoch: 16/100... Etape: 2850... Loss: 2.3454... Val Loss: 2.3646\n",
            "Epoch: 16/100... Etape: 2860... Loss: 2.3882... Val Loss: 2.3629\n",
            "Epoch: 16/100... Etape: 2870... Loss: 2.3385... Val Loss: 2.3613\n",
            "Epoch: 16/100... Etape: 2880... Loss: 2.3847... Val Loss: 2.3633\n",
            "Epoch: 16/100... Etape: 2890... Loss: 2.4032... Val Loss: 2.3610\n",
            "Epoch: 16/100... Etape: 2900... Loss: 2.3807... Val Loss: 2.3601\n",
            "Epoch: 16/100... Etape: 2910... Loss: 2.3357... Val Loss: 2.3600\n",
            "Epoch: 16/100... Etape: 2920... Loss: 2.3946... Val Loss: 2.3574\n",
            "Epoch: 16/100... Etape: 2930... Loss: 2.4053... Val Loss: 2.3596\n",
            "Epoch: 16/100... Etape: 2940... Loss: 2.3570... Val Loss: 2.3560\n",
            "Epoch: 16/100... Etape: 2950... Loss: 2.3708... Val Loss: 2.3537\n",
            "Epoch: 16/100... Etape: 2960... Loss: 2.3968... Val Loss: 2.3539\n",
            "Epoch: 16/100... Etape: 2970... Loss: 2.3282... Val Loss: 2.3545\n",
            "Epoch: 17/100... Etape: 2980... Loss: 2.3676... Val Loss: 2.3561\n",
            "Epoch: 17/100... Etape: 2990... Loss: 2.3424... Val Loss: 2.3549\n",
            "Epoch: 17/100... Etape: 3000... Loss: 2.3428... Val Loss: 2.3542\n",
            "Epoch: 17/100... Etape: 3010... Loss: 2.3796... Val Loss: 2.3530\n",
            "Epoch: 17/100... Etape: 3020... Loss: 2.3341... Val Loss: 2.3516\n",
            "Epoch: 17/100... Etape: 3030... Loss: 2.3541... Val Loss: 2.3546\n",
            "Epoch: 17/100... Etape: 3040... Loss: 2.3563... Val Loss: 2.3530\n",
            "Epoch: 17/100... Etape: 3050... Loss: 2.3897... Val Loss: 2.3465\n",
            "Epoch: 17/100... Etape: 3060... Loss: 2.3813... Val Loss: 2.3461\n",
            "Epoch: 17/100... Etape: 3070... Loss: 2.4126... Val Loss: 2.3479\n",
            "Epoch: 17/100... Etape: 3080... Loss: 2.3655... Val Loss: 2.3474\n",
            "Epoch: 17/100... Etape: 3090... Loss: 2.3610... Val Loss: 2.3460\n",
            "Epoch: 17/100... Etape: 3100... Loss: 2.3918... Val Loss: 2.3466\n",
            "Epoch: 17/100... Etape: 3110... Loss: 2.3504... Val Loss: 2.3456\n",
            "Epoch: 17/100... Etape: 3120... Loss: 2.3824... Val Loss: 2.3445\n",
            "Epoch: 17/100... Etape: 3130... Loss: 2.3491... Val Loss: 2.3431\n",
            "Epoch: 17/100... Etape: 3140... Loss: 2.3273... Val Loss: 2.3423\n",
            "Epoch: 17/100... Etape: 3150... Loss: 2.2937... Val Loss: 2.3421\n",
            "Epoch: 17/100... Etape: 3160... Loss: 2.3919... Val Loss: 2.3441\n",
            "Epoch: 18/100... Etape: 3170... Loss: 2.3418... Val Loss: 2.3414\n",
            "Epoch: 18/100... Etape: 3180... Loss: 2.3275... Val Loss: 2.3392\n",
            "Epoch: 18/100... Etape: 3190... Loss: 2.3714... Val Loss: 2.3419\n",
            "Epoch: 18/100... Etape: 3200... Loss: 2.3349... Val Loss: 2.3386\n",
            "Epoch: 18/100... Etape: 3210... Loss: 2.3411... Val Loss: 2.3398\n",
            "Epoch: 18/100... Etape: 3220... Loss: 2.3526... Val Loss: 2.3393\n",
            "Epoch: 18/100... Etape: 3230... Loss: 2.3161... Val Loss: 2.3387\n",
            "Epoch: 18/100... Etape: 3240... Loss: 2.3469... Val Loss: 2.3320\n",
            "Epoch: 18/100... Etape: 3250... Loss: 2.3234... Val Loss: 2.3359\n",
            "Epoch: 18/100... Etape: 3260... Loss: 2.3271... Val Loss: 2.3348\n",
            "Epoch: 18/100... Etape: 3270... Loss: 2.3927... Val Loss: 2.3345\n",
            "Epoch: 18/100... Etape: 3280... Loss: 2.3771... Val Loss: 2.3345\n",
            "Epoch: 18/100... Etape: 3290... Loss: 2.3494... Val Loss: 2.3339\n",
            "Epoch: 18/100... Etape: 3300... Loss: 2.3510... Val Loss: 2.3344\n",
            "Epoch: 18/100... Etape: 3310... Loss: 2.3521... Val Loss: 2.3327\n",
            "Epoch: 18/100... Etape: 3320... Loss: 2.3443... Val Loss: 2.3275\n",
            "Epoch: 18/100... Etape: 3330... Loss: 2.3302... Val Loss: 2.3277\n",
            "Epoch: 18/100... Etape: 3340... Loss: 2.3739... Val Loss: 2.3302\n",
            "Epoch: 19/100... Etape: 3350... Loss: 2.3461... Val Loss: 2.3324\n",
            "Epoch: 19/100... Etape: 3360... Loss: 2.3218... Val Loss: 2.3315\n",
            "Epoch: 19/100... Etape: 3370... Loss: 2.2977... Val Loss: 2.3279\n",
            "Epoch: 19/100... Etape: 3380... Loss: 2.4071... Val Loss: 2.3283\n",
            "Epoch: 19/100... Etape: 3390... Loss: 2.3264... Val Loss: 2.3261\n",
            "Epoch: 19/100... Etape: 3400... Loss: 2.3510... Val Loss: 2.3288\n",
            "Epoch: 19/100... Etape: 3410... Loss: 2.3523... Val Loss: 2.3265\n",
            "Epoch: 19/100... Etape: 3420... Loss: 2.3348... Val Loss: 2.3240\n",
            "Epoch: 19/100... Etape: 3430... Loss: 2.3089... Val Loss: 2.3209\n",
            "Epoch: 19/100... Etape: 3440... Loss: 2.3798... Val Loss: 2.3246\n",
            "Epoch: 19/100... Etape: 3450... Loss: 2.3901... Val Loss: 2.3237\n",
            "Epoch: 19/100... Etape: 3460... Loss: 2.3140... Val Loss: 2.3249\n",
            "Epoch: 19/100... Etape: 3470... Loss: 2.3354... Val Loss: 2.3212\n",
            "Epoch: 19/100... Etape: 3480... Loss: 2.3448... Val Loss: 2.3211\n",
            "Epoch: 19/100... Etape: 3490... Loss: 2.3611... Val Loss: 2.3206\n",
            "Epoch: 19/100... Etape: 3500... Loss: 2.3434... Val Loss: 2.3220\n",
            "Epoch: 19/100... Etape: 3510... Loss: 2.3353... Val Loss: 2.3202\n",
            "Epoch: 19/100... Etape: 3520... Loss: 2.3181... Val Loss: 2.3152\n",
            "Epoch: 19/100... Etape: 3530... Loss: 2.3321... Val Loss: 2.3198\n",
            "Epoch: 20/100... Etape: 3540... Loss: 2.3504... Val Loss: 2.3205\n",
            "Epoch: 20/100... Etape: 3550... Loss: 2.3113... Val Loss: 2.3171\n",
            "Epoch: 20/100... Etape: 3560... Loss: 2.3505... Val Loss: 2.3169\n",
            "Epoch: 20/100... Etape: 3570... Loss: 2.3485... Val Loss: 2.3147\n",
            "Epoch: 20/100... Etape: 3580... Loss: 2.2804... Val Loss: 2.3153\n",
            "Epoch: 20/100... Etape: 3590... Loss: 2.2953... Val Loss: 2.3146\n",
            "Epoch: 20/100... Etape: 3600... Loss: 2.3180... Val Loss: 2.3170\n",
            "Epoch: 20/100... Etape: 3610... Loss: 2.3240... Val Loss: 2.3098\n",
            "Epoch: 20/100... Etape: 3620... Loss: 2.2889... Val Loss: 2.3125\n",
            "Epoch: 20/100... Etape: 3630... Loss: 2.3281... Val Loss: 2.3130\n",
            "Epoch: 20/100... Etape: 3640... Loss: 2.3287... Val Loss: 2.3106\n",
            "Epoch: 20/100... Etape: 3650... Loss: 2.3547... Val Loss: 2.3126\n",
            "Epoch: 20/100... Etape: 3660... Loss: 2.3132... Val Loss: 2.3112\n",
            "Epoch: 20/100... Etape: 3670... Loss: 2.3442... Val Loss: 2.3114\n",
            "Epoch: 20/100... Etape: 3680... Loss: 2.2986... Val Loss: 2.3104\n",
            "Epoch: 20/100... Etape: 3690... Loss: 2.3351... Val Loss: 2.3077\n",
            "Epoch: 20/100... Etape: 3700... Loss: 2.3023... Val Loss: 2.3100\n",
            "Epoch: 20/100... Etape: 3710... Loss: 2.3129... Val Loss: 2.3083\n",
            "Epoch: 20/100... Etape: 3720... Loss: 2.3253... Val Loss: 2.3099\n",
            "Epoch: 21/100... Etape: 3730... Loss: 2.3493... Val Loss: 2.3110\n",
            "Epoch: 21/100... Etape: 3740... Loss: 2.2873... Val Loss: 2.3054\n",
            "Epoch: 21/100... Etape: 3750... Loss: 2.3058... Val Loss: 2.3061\n",
            "Epoch: 21/100... Etape: 3760... Loss: 2.2945... Val Loss: 2.3068\n",
            "Epoch: 21/100... Etape: 3770... Loss: 2.2933... Val Loss: 2.3091\n",
            "Epoch: 21/100... Etape: 3780... Loss: 2.2926... Val Loss: 2.3061\n",
            "Epoch: 21/100... Etape: 3790... Loss: 2.3376... Val Loss: 2.3036\n",
            "Epoch: 21/100... Etape: 3800... Loss: 2.2777... Val Loss: 2.3011\n",
            "Epoch: 21/100... Etape: 3810... Loss: 2.3139... Val Loss: 2.3048\n",
            "Epoch: 21/100... Etape: 3820... Loss: 2.3396... Val Loss: 2.3029\n",
            "Epoch: 21/100... Etape: 3830... Loss: 2.3282... Val Loss: 2.3031\n",
            "Epoch: 21/100... Etape: 3840... Loss: 2.2832... Val Loss: 2.3013\n",
            "Epoch: 21/100... Etape: 3850... Loss: 2.3419... Val Loss: 2.3022\n",
            "Epoch: 21/100... Etape: 3860... Loss: 2.3370... Val Loss: 2.3007\n",
            "Epoch: 21/100... Etape: 3870... Loss: 2.3006... Val Loss: 2.3029\n",
            "Epoch: 21/100... Etape: 3880... Loss: 2.3162... Val Loss: 2.2977\n",
            "Epoch: 21/100... Etape: 3890... Loss: 2.3327... Val Loss: 2.2976\n",
            "Epoch: 21/100... Etape: 3900... Loss: 2.2785... Val Loss: 2.3013\n",
            "Epoch: 22/100... Etape: 3910... Loss: 2.3117... Val Loss: 2.3003\n",
            "Epoch: 22/100... Etape: 3920... Loss: 2.2982... Val Loss: 2.2987\n",
            "Epoch: 22/100... Etape: 3930... Loss: 2.2735... Val Loss: 2.2979\n",
            "Epoch: 22/100... Etape: 3940... Loss: 2.3363... Val Loss: 2.2996\n",
            "Epoch: 22/100... Etape: 3950... Loss: 2.2757... Val Loss: 2.2971\n",
            "Epoch: 22/100... Etape: 3960... Loss: 2.3091... Val Loss: 2.2996\n",
            "Epoch: 22/100... Etape: 3970... Loss: 2.3086... Val Loss: 2.2979\n",
            "Epoch: 22/100... Etape: 3980... Loss: 2.3344... Val Loss: 2.2925\n",
            "Epoch: 22/100... Etape: 3990... Loss: 2.3372... Val Loss: 2.2908\n",
            "Epoch: 22/100... Etape: 4000... Loss: 2.3662... Val Loss: 2.2956\n",
            "Epoch: 22/100... Etape: 4010... Loss: 2.3248... Val Loss: 2.2931\n",
            "Epoch: 22/100... Etape: 4020... Loss: 2.2993... Val Loss: 2.2943\n",
            "Epoch: 22/100... Etape: 4030... Loss: 2.3311... Val Loss: 2.2937\n",
            "Epoch: 22/100... Etape: 4040... Loss: 2.3124... Val Loss: 2.2949\n",
            "Epoch: 22/100... Etape: 4050... Loss: 2.3409... Val Loss: 2.2915\n",
            "Epoch: 22/100... Etape: 4060... Loss: 2.3057... Val Loss: 2.2954\n",
            "Epoch: 22/100... Etape: 4070... Loss: 2.2815... Val Loss: 2.2926\n",
            "Epoch: 22/100... Etape: 4080... Loss: 2.2598... Val Loss: 2.2890\n",
            "Epoch: 22/100... Etape: 4090... Loss: 2.3621... Val Loss: 2.2937\n",
            "Epoch: 23/100... Etape: 4100... Loss: 2.2904... Val Loss: 2.2897\n",
            "Epoch: 23/100... Etape: 4110... Loss: 2.2819... Val Loss: 2.2888\n",
            "Epoch: 23/100... Etape: 4120... Loss: 2.3246... Val Loss: 2.2902\n",
            "Epoch: 23/100... Etape: 4130... Loss: 2.2903... Val Loss: 2.2887\n",
            "Epoch: 23/100... Etape: 4140... Loss: 2.2858... Val Loss: 2.2896\n",
            "Epoch: 23/100... Etape: 4150... Loss: 2.3185... Val Loss: 2.2873\n",
            "Epoch: 23/100... Etape: 4160... Loss: 2.2592... Val Loss: 2.2902\n",
            "Epoch: 23/100... Etape: 4170... Loss: 2.3088... Val Loss: 2.2831\n",
            "Epoch: 23/100... Etape: 4180... Loss: 2.2851... Val Loss: 2.2855\n",
            "Epoch: 23/100... Etape: 4190... Loss: 2.2848... Val Loss: 2.2853\n",
            "Epoch: 23/100... Etape: 4200... Loss: 2.3556... Val Loss: 2.2843\n",
            "Epoch: 23/100... Etape: 4210... Loss: 2.3429... Val Loss: 2.2863\n",
            "Epoch: 23/100... Etape: 4220... Loss: 2.3026... Val Loss: 2.2851\n",
            "Epoch: 23/100... Etape: 4230... Loss: 2.3197... Val Loss: 2.2846\n",
            "Epoch: 23/100... Etape: 4240... Loss: 2.3109... Val Loss: 2.2888\n",
            "Epoch: 23/100... Etape: 4250... Loss: 2.2878... Val Loss: 2.2813\n",
            "Epoch: 23/100... Etape: 4260... Loss: 2.2775... Val Loss: 2.2817\n",
            "Epoch: 23/100... Etape: 4270... Loss: 2.3108... Val Loss: 2.2841\n",
            "Epoch: 24/100... Etape: 4280... Loss: 2.2943... Val Loss: 2.2827\n",
            "Epoch: 24/100... Etape: 4290... Loss: 2.2850... Val Loss: 2.2867\n",
            "Epoch: 24/100... Etape: 4300... Loss: 2.2534... Val Loss: 2.2808\n",
            "Epoch: 24/100... Etape: 4310... Loss: 2.3482... Val Loss: 2.2813\n",
            "Epoch: 24/100... Etape: 4320... Loss: 2.2731... Val Loss: 2.2847\n",
            "Epoch: 24/100... Etape: 4330... Loss: 2.3202... Val Loss: 2.2856\n",
            "Epoch: 24/100... Etape: 4340... Loss: 2.3132... Val Loss: 2.2811\n",
            "Epoch: 24/100... Etape: 4350... Loss: 2.2782... Val Loss: 2.2799\n",
            "Epoch: 24/100... Etape: 4360... Loss: 2.2693... Val Loss: 2.2763\n",
            "Epoch: 24/100... Etape: 4370... Loss: 2.3263... Val Loss: 2.2769\n",
            "Epoch: 24/100... Etape: 4380... Loss: 2.3387... Val Loss: 2.2786\n",
            "Epoch: 24/100... Etape: 4390... Loss: 2.2504... Val Loss: 2.2783\n",
            "Epoch: 24/100... Etape: 4400... Loss: 2.2983... Val Loss: 2.2768\n",
            "Epoch: 24/100... Etape: 4410... Loss: 2.3177... Val Loss: 2.2785\n",
            "Epoch: 24/100... Etape: 4420... Loss: 2.3215... Val Loss: 2.2781\n",
            "Epoch: 24/100... Etape: 4430... Loss: 2.2978... Val Loss: 2.2795\n",
            "Epoch: 24/100... Etape: 4440... Loss: 2.2701... Val Loss: 2.2762\n",
            "Epoch: 24/100... Etape: 4450... Loss: 2.2764... Val Loss: 2.2734\n",
            "Epoch: 24/100... Etape: 4460... Loss: 2.2955... Val Loss: 2.2798\n",
            "Epoch: 25/100... Etape: 4470... Loss: 2.3125... Val Loss: 2.2771\n",
            "Epoch: 25/100... Etape: 4480... Loss: 2.2748... Val Loss: 2.2746\n",
            "Epoch: 25/100... Etape: 4490... Loss: 2.3090... Val Loss: 2.2753\n",
            "Epoch: 25/100... Etape: 4500... Loss: 2.2861... Val Loss: 2.2741\n",
            "Epoch: 25/100... Etape: 4510... Loss: 2.2318... Val Loss: 2.2755\n",
            "Epoch: 25/100... Etape: 4520... Loss: 2.2449... Val Loss: 2.2766\n",
            "Epoch: 25/100... Etape: 4530... Loss: 2.2926... Val Loss: 2.2770\n",
            "Epoch: 25/100... Etape: 4540... Loss: 2.2743... Val Loss: 2.2695\n",
            "Epoch: 25/100... Etape: 4550... Loss: 2.2377... Val Loss: 2.2716\n",
            "Epoch: 25/100... Etape: 4560... Loss: 2.2879... Val Loss: 2.2709\n",
            "Epoch: 25/100... Etape: 4570... Loss: 2.2891... Val Loss: 2.2707\n",
            "Epoch: 25/100... Etape: 4580... Loss: 2.3075... Val Loss: 2.2727\n",
            "Epoch: 25/100... Etape: 4590... Loss: 2.2698... Val Loss: 2.2702\n",
            "Epoch: 25/100... Etape: 4600... Loss: 2.2963... Val Loss: 2.2707\n",
            "Epoch: 25/100... Etape: 4610... Loss: 2.2608... Val Loss: 2.2688\n",
            "Epoch: 25/100... Etape: 4620... Loss: 2.2978... Val Loss: 2.2676\n",
            "Epoch: 25/100... Etape: 4630... Loss: 2.2763... Val Loss: 2.2691\n",
            "Epoch: 25/100... Etape: 4640... Loss: 2.2868... Val Loss: 2.2686\n",
            "Epoch: 25/100... Etape: 4650... Loss: 2.2843... Val Loss: 2.2713\n",
            "Epoch: 26/100... Etape: 4660... Loss: 2.3065... Val Loss: 2.2727\n",
            "Epoch: 26/100... Etape: 4670... Loss: 2.2525... Val Loss: 2.2685\n",
            "Epoch: 26/100... Etape: 4680... Loss: 2.2701... Val Loss: 2.2696\n",
            "Epoch: 26/100... Etape: 4690... Loss: 2.2430... Val Loss: 2.2670\n",
            "Epoch: 26/100... Etape: 4700... Loss: 2.2544... Val Loss: 2.2703\n",
            "Epoch: 26/100... Etape: 4710... Loss: 2.2456... Val Loss: 2.2684\n",
            "Epoch: 26/100... Etape: 4720... Loss: 2.2900... Val Loss: 2.2637\n",
            "Epoch: 26/100... Etape: 4730... Loss: 2.2398... Val Loss: 2.2629\n",
            "Epoch: 26/100... Etape: 4740... Loss: 2.2912... Val Loss: 2.2643\n",
            "Epoch: 26/100... Etape: 4750... Loss: 2.2914... Val Loss: 2.2616\n",
            "Epoch: 26/100... Etape: 4760... Loss: 2.3013... Val Loss: 2.2646\n",
            "Epoch: 26/100... Etape: 4770... Loss: 2.2509... Val Loss: 2.2647\n",
            "Epoch: 26/100... Etape: 4780... Loss: 2.3135... Val Loss: 2.2616\n",
            "Epoch: 26/100... Etape: 4790... Loss: 2.2905... Val Loss: 2.2614\n",
            "Epoch: 26/100... Etape: 4800... Loss: 2.2801... Val Loss: 2.2665\n",
            "Epoch: 26/100... Etape: 4810... Loss: 2.2613... Val Loss: 2.2597\n",
            "Epoch: 26/100... Etape: 4820... Loss: 2.2969... Val Loss: 2.2659\n",
            "Epoch: 26/100... Etape: 4830... Loss: 2.2265... Val Loss: 2.2616\n",
            "Epoch: 27/100... Etape: 4840... Loss: 2.2704... Val Loss: 2.2652\n",
            "Epoch: 27/100... Etape: 4850... Loss: 2.2507... Val Loss: 2.2643\n",
            "Epoch: 27/100... Etape: 4860... Loss: 2.2339... Val Loss: 2.2585\n",
            "Epoch: 27/100... Etape: 4870... Loss: 2.2797... Val Loss: 2.2623\n",
            "Epoch: 27/100... Etape: 4880... Loss: 2.2367... Val Loss: 2.2629\n",
            "Epoch: 27/100... Etape: 4890... Loss: 2.2553... Val Loss: 2.2646\n",
            "Epoch: 27/100... Etape: 4900... Loss: 2.2601... Val Loss: 2.2609\n",
            "Epoch: 27/100... Etape: 4910... Loss: 2.2978... Val Loss: 2.2583\n",
            "Epoch: 27/100... Etape: 4920... Loss: 2.2981... Val Loss: 2.2559\n",
            "Epoch: 27/100... Etape: 4930... Loss: 2.3282... Val Loss: 2.2554\n",
            "Epoch: 27/100... Etape: 4940... Loss: 2.2810... Val Loss: 2.2561\n",
            "Epoch: 27/100... Etape: 4950... Loss: 2.2773... Val Loss: 2.2604\n",
            "Epoch: 27/100... Etape: 4960... Loss: 2.2904... Val Loss: 2.2579\n",
            "Epoch: 27/100... Etape: 4970... Loss: 2.2668... Val Loss: 2.2607\n",
            "Epoch: 27/100... Etape: 4980... Loss: 2.3084... Val Loss: 2.2550\n",
            "Epoch: 27/100... Etape: 4990... Loss: 2.2746... Val Loss: 2.2587\n",
            "Epoch: 27/100... Etape: 5000... Loss: 2.2412... Val Loss: 2.2554\n",
            "Epoch: 27/100... Etape: 5010... Loss: 2.2208... Val Loss: 2.2587\n",
            "Epoch: 27/100... Etape: 5020... Loss: 2.3144... Val Loss: 2.2575\n",
            "Epoch: 28/100... Etape: 5030... Loss: 2.2673... Val Loss: 2.2587\n",
            "Epoch: 28/100... Etape: 5040... Loss: 2.2379... Val Loss: 2.2539\n",
            "Epoch: 28/100... Etape: 5050... Loss: 2.2877... Val Loss: 2.2586\n",
            "Epoch: 28/100... Etape: 5060... Loss: 2.2427... Val Loss: 2.2568\n",
            "Epoch: 28/100... Etape: 5070... Loss: 2.2578... Val Loss: 2.2574\n",
            "Epoch: 28/100... Etape: 5080... Loss: 2.2677... Val Loss: 2.2554\n",
            "Epoch: 28/100... Etape: 5090... Loss: 2.2279... Val Loss: 2.2527\n",
            "Epoch: 28/100... Etape: 5100... Loss: 2.2532... Val Loss: 2.2494\n",
            "Epoch: 28/100... Etape: 5110... Loss: 2.2532... Val Loss: 2.2504\n",
            "Epoch: 28/100... Etape: 5120... Loss: 2.2567... Val Loss: 2.2492\n",
            "Epoch: 28/100... Etape: 5130... Loss: 2.3231... Val Loss: 2.2501\n",
            "Epoch: 28/100... Etape: 5140... Loss: 2.2950... Val Loss: 2.2558\n",
            "Epoch: 28/100... Etape: 5150... Loss: 2.2674... Val Loss: 2.2509\n",
            "Epoch: 28/100... Etape: 5160... Loss: 2.2682... Val Loss: 2.2504\n",
            "Epoch: 28/100... Etape: 5170... Loss: 2.2780... Val Loss: 2.2495\n",
            "Epoch: 28/100... Etape: 5180... Loss: 2.2552... Val Loss: 2.2483\n",
            "Epoch: 28/100... Etape: 5190... Loss: 2.2507... Val Loss: 2.2518\n",
            "Epoch: 28/100... Etape: 5200... Loss: 2.2814... Val Loss: 2.2477\n",
            "Epoch: 29/100... Etape: 5210... Loss: 2.2630... Val Loss: 2.2552\n",
            "Epoch: 29/100... Etape: 5220... Loss: 2.2319... Val Loss: 2.2498\n",
            "Epoch: 29/100... Etape: 5230... Loss: 2.2122... Val Loss: 2.2470\n",
            "Epoch: 29/100... Etape: 5240... Loss: 2.3022... Val Loss: 2.2531\n",
            "Epoch: 29/100... Etape: 5250... Loss: 2.2426... Val Loss: 2.2463\n",
            "Epoch: 29/100... Etape: 5260... Loss: 2.2955... Val Loss: 2.2542\n",
            "Epoch: 29/100... Etape: 5270... Loss: 2.2751... Val Loss: 2.2480\n",
            "Epoch: 29/100... Etape: 5280... Loss: 2.2491... Val Loss: 2.2471\n",
            "Epoch: 29/100... Etape: 5290... Loss: 2.2197... Val Loss: 2.2452\n",
            "Epoch: 29/100... Etape: 5300... Loss: 2.2771... Val Loss: 2.2444\n",
            "Epoch: 29/100... Etape: 5310... Loss: 2.3034... Val Loss: 2.2440\n",
            "Epoch: 29/100... Etape: 5320... Loss: 2.2267... Val Loss: 2.2470\n",
            "Epoch: 29/100... Etape: 5330... Loss: 2.2640... Val Loss: 2.2477\n",
            "Epoch: 29/100... Etape: 5340... Loss: 2.2781... Val Loss: 2.2467\n",
            "Epoch: 29/100... Etape: 5350... Loss: 2.2865... Val Loss: 2.2438\n",
            "Epoch: 29/100... Etape: 5360... Loss: 2.2705... Val Loss: 2.2488\n",
            "Epoch: 29/100... Etape: 5370... Loss: 2.2483... Val Loss: 2.2411\n",
            "Epoch: 29/100... Etape: 5380... Loss: 2.2418... Val Loss: 2.2453\n",
            "Epoch: 29/100... Etape: 5390... Loss: 2.2457... Val Loss: 2.2431\n",
            "Epoch: 30/100... Etape: 5400... Loss: 2.2778... Val Loss: 2.2488\n",
            "Epoch: 30/100... Etape: 5410... Loss: 2.2202... Val Loss: 2.2426\n",
            "Epoch: 30/100... Etape: 5420... Loss: 2.2755... Val Loss: 2.2427\n",
            "Epoch: 30/100... Etape: 5430... Loss: 2.2636... Val Loss: 2.2444\n",
            "Epoch: 30/100... Etape: 5440... Loss: 2.1897... Val Loss: 2.2454\n",
            "Epoch: 30/100... Etape: 5450... Loss: 2.2135... Val Loss: 2.2472\n",
            "Epoch: 30/100... Etape: 5460... Loss: 2.2347... Val Loss: 2.2424\n",
            "Epoch: 30/100... Etape: 5470... Loss: 2.2435... Val Loss: 2.2391\n",
            "Epoch: 30/100... Etape: 5480... Loss: 2.2044... Val Loss: 2.2374\n",
            "Epoch: 30/100... Etape: 5490... Loss: 2.2399... Val Loss: 2.2410\n",
            "Epoch: 30/100... Etape: 5500... Loss: 2.2516... Val Loss: 2.2395\n",
            "Epoch: 30/100... Etape: 5510... Loss: 2.2641... Val Loss: 2.2412\n",
            "Epoch: 30/100... Etape: 5520... Loss: 2.2225... Val Loss: 2.2418\n",
            "Epoch: 30/100... Etape: 5530... Loss: 2.2633... Val Loss: 2.2435\n",
            "Epoch: 30/100... Etape: 5540... Loss: 2.2418... Val Loss: 2.2387\n",
            "Epoch: 30/100... Etape: 5550... Loss: 2.2526... Val Loss: 2.2400\n",
            "Epoch: 30/100... Etape: 5560... Loss: 2.2315... Val Loss: 2.2387\n",
            "Epoch: 30/100... Etape: 5570... Loss: 2.2523... Val Loss: 2.2392\n",
            "Epoch: 30/100... Etape: 5580... Loss: 2.2508... Val Loss: 2.2419\n",
            "Epoch: 31/100... Etape: 5590... Loss: 2.2645... Val Loss: 2.2462\n",
            "Epoch: 31/100... Etape: 5600... Loss: 2.2099... Val Loss: 2.2351\n",
            "Epoch: 31/100... Etape: 5610... Loss: 2.2345... Val Loss: 2.2422\n",
            "Epoch: 31/100... Etape: 5620... Loss: 2.1887... Val Loss: 2.2372\n",
            "Epoch: 31/100... Etape: 5630... Loss: 2.2211... Val Loss: 2.2439\n",
            "Epoch: 31/100... Etape: 5640... Loss: 2.2070... Val Loss: 2.2384\n",
            "Epoch: 31/100... Etape: 5650... Loss: 2.2580... Val Loss: 2.2356\n",
            "Epoch: 31/100... Etape: 5660... Loss: 2.2053... Val Loss: 2.2349\n",
            "Epoch: 31/100... Etape: 5670... Loss: 2.2474... Val Loss: 2.2360\n",
            "Epoch: 31/100... Etape: 5680... Loss: 2.2749... Val Loss: 2.2337\n",
            "Epoch: 31/100... Etape: 5690... Loss: 2.2540... Val Loss: 2.2352\n",
            "Epoch: 31/100... Etape: 5700... Loss: 2.2189... Val Loss: 2.2381\n",
            "Epoch: 31/100... Etape: 5710... Loss: 2.2585... Val Loss: 2.2343\n",
            "Epoch: 31/100... Etape: 5720... Loss: 2.2579... Val Loss: 2.2344\n",
            "Epoch: 31/100... Etape: 5730... Loss: 2.2415... Val Loss: 2.2352\n",
            "Epoch: 31/100... Etape: 5740... Loss: 2.2321... Val Loss: 2.2315\n",
            "Epoch: 31/100... Etape: 5750... Loss: 2.2692... Val Loss: 2.2350\n",
            "Epoch: 31/100... Etape: 5760... Loss: 2.2001... Val Loss: 2.2343\n",
            "Epoch: 32/100... Etape: 5770... Loss: 2.2429... Val Loss: 2.2429\n",
            "Epoch: 32/100... Etape: 5780... Loss: 2.2106... Val Loss: 2.2353\n",
            "Epoch: 32/100... Etape: 5790... Loss: 2.2018... Val Loss: 2.2317\n",
            "Epoch: 32/100... Etape: 5800... Loss: 2.2491... Val Loss: 2.2366\n",
            "Epoch: 32/100... Etape: 5810... Loss: 2.2005... Val Loss: 2.2323\n",
            "Epoch: 32/100... Etape: 5820... Loss: 2.2361... Val Loss: 2.2386\n",
            "Epoch: 32/100... Etape: 5830... Loss: 2.2270... Val Loss: 2.2323\n",
            "Epoch: 32/100... Etape: 5840... Loss: 2.2595... Val Loss: 2.2316\n",
            "Epoch: 32/100... Etape: 5850... Loss: 2.2742... Val Loss: 2.2275\n",
            "Epoch: 32/100... Etape: 5860... Loss: 2.2910... Val Loss: 2.2275\n",
            "Epoch: 32/100... Etape: 5870... Loss: 2.2541... Val Loss: 2.2274\n",
            "Epoch: 32/100... Etape: 5880... Loss: 2.2395... Val Loss: 2.2327\n",
            "Epoch: 32/100... Etape: 5890... Loss: 2.2750... Val Loss: 2.2314\n",
            "Epoch: 32/100... Etape: 5900... Loss: 2.2457... Val Loss: 2.2298\n",
            "Epoch: 32/100... Etape: 5910... Loss: 2.2746... Val Loss: 2.2261\n",
            "Epoch: 32/100... Etape: 5920... Loss: 2.2450... Val Loss: 2.2356\n",
            "Epoch: 32/100... Etape: 5930... Loss: 2.2108... Val Loss: 2.2270\n",
            "Epoch: 32/100... Etape: 5940... Loss: 2.1719... Val Loss: 2.2318\n",
            "Epoch: 32/100... Etape: 5950... Loss: 2.3065... Val Loss: 2.2289\n",
            "Epoch: 33/100... Etape: 5960... Loss: 2.2165... Val Loss: 2.2345\n",
            "Epoch: 33/100... Etape: 5970... Loss: 2.2188... Val Loss: 2.2259\n",
            "Epoch: 33/100... Etape: 5980... Loss: 2.2525... Val Loss: 2.2295\n",
            "Epoch: 33/100... Etape: 5990... Loss: 2.2148... Val Loss: 2.2308\n",
            "Epoch: 33/100... Etape: 6000... Loss: 2.2247... Val Loss: 2.2279\n",
            "Epoch: 33/100... Etape: 6010... Loss: 2.2409... Val Loss: 2.2315\n",
            "Epoch: 33/100... Etape: 6020... Loss: 2.1995... Val Loss: 2.2255\n",
            "Epoch: 33/100... Etape: 6030... Loss: 2.2084... Val Loss: 2.2239\n",
            "Epoch: 33/100... Etape: 6040... Loss: 2.2209... Val Loss: 2.2232\n",
            "Epoch: 33/100... Etape: 6050... Loss: 2.2068... Val Loss: 2.2243\n",
            "Epoch: 33/100... Etape: 6060... Loss: 2.2956... Val Loss: 2.2236\n",
            "Epoch: 33/100... Etape: 6070... Loss: 2.2806... Val Loss: 2.2253\n",
            "Epoch: 33/100... Etape: 6080... Loss: 2.2310... Val Loss: 2.2253\n",
            "Epoch: 33/100... Etape: 6090... Loss: 2.2382... Val Loss: 2.2240\n",
            "Epoch: 33/100... Etape: 6100... Loss: 2.2627... Val Loss: 2.2224\n",
            "Epoch: 33/100... Etape: 6110... Loss: 2.2141... Val Loss: 2.2244\n",
            "Epoch: 33/100... Etape: 6120... Loss: 2.2141... Val Loss: 2.2229\n",
            "Epoch: 33/100... Etape: 6130... Loss: 2.2525... Val Loss: 2.2226\n",
            "Epoch: 34/100... Etape: 6140... Loss: 2.2418... Val Loss: 2.2264\n",
            "Epoch: 34/100... Etape: 6150... Loss: 2.2077... Val Loss: 2.2271\n",
            "Epoch: 34/100... Etape: 6160... Loss: 2.1832... Val Loss: 2.2195\n",
            "Epoch: 34/100... Etape: 6170... Loss: 2.2807... Val Loss: 2.2268\n",
            "Epoch: 34/100... Etape: 6180... Loss: 2.1929... Val Loss: 2.2199\n",
            "Epoch: 34/100... Etape: 6190... Loss: 2.2560... Val Loss: 2.2280\n",
            "Epoch: 34/100... Etape: 6200... Loss: 2.2424... Val Loss: 2.2216\n",
            "Epoch: 34/100... Etape: 6210... Loss: 2.2200... Val Loss: 2.2180\n",
            "Epoch: 34/100... Etape: 6220... Loss: 2.2145... Val Loss: 2.2213\n",
            "Epoch: 34/100... Etape: 6230... Loss: 2.2616... Val Loss: 2.2187\n",
            "Epoch: 34/100... Etape: 6240... Loss: 2.2726... Val Loss: 2.2169\n",
            "Epoch: 34/100... Etape: 6250... Loss: 2.1984... Val Loss: 2.2213\n",
            "Epoch: 34/100... Etape: 6260... Loss: 2.2217... Val Loss: 2.2199\n",
            "Epoch: 34/100... Etape: 6270... Loss: 2.2576... Val Loss: 2.2189\n",
            "Epoch: 34/100... Etape: 6280... Loss: 2.2630... Val Loss: 2.2174\n",
            "Epoch: 34/100... Etape: 6290... Loss: 2.2265... Val Loss: 2.2202\n",
            "Epoch: 34/100... Etape: 6300... Loss: 2.2204... Val Loss: 2.2162\n",
            "Epoch: 34/100... Etape: 6310... Loss: 2.2147... Val Loss: 2.2190\n",
            "Epoch: 34/100... Etape: 6320... Loss: 2.2043... Val Loss: 2.2196\n",
            "Epoch: 35/100... Etape: 6330... Loss: 2.2397... Val Loss: 2.2267\n",
            "Epoch: 35/100... Etape: 6340... Loss: 2.1765... Val Loss: 2.2179\n",
            "Epoch: 35/100... Etape: 6350... Loss: 2.2415... Val Loss: 2.2194\n",
            "Epoch: 35/100... Etape: 6360... Loss: 2.2239... Val Loss: 2.2208\n",
            "Epoch: 35/100... Etape: 6370... Loss: 2.1580... Val Loss: 2.2210\n",
            "Epoch: 35/100... Etape: 6380... Loss: 2.1992... Val Loss: 2.2227\n",
            "Epoch: 35/100... Etape: 6390... Loss: 2.2119... Val Loss: 2.2181\n",
            "Epoch: 35/100... Etape: 6400... Loss: 2.2280... Val Loss: 2.2132\n",
            "Epoch: 35/100... Etape: 6410... Loss: 2.1876... Val Loss: 2.2133\n",
            "Epoch: 35/100... Etape: 6420... Loss: 2.2117... Val Loss: 2.2130\n",
            "Epoch: 35/100... Etape: 6430... Loss: 2.2319... Val Loss: 2.2147\n",
            "Epoch: 35/100... Etape: 6440... Loss: 2.2402... Val Loss: 2.2180\n",
            "Epoch: 35/100... Etape: 6450... Loss: 2.1907... Val Loss: 2.2200\n",
            "Epoch: 35/100... Etape: 6460... Loss: 2.2298... Val Loss: 2.2151\n",
            "Epoch: 35/100... Etape: 6470... Loss: 2.1947... Val Loss: 2.2117\n",
            "Epoch: 35/100... Etape: 6480... Loss: 2.2170... Val Loss: 2.2182\n",
            "Epoch: 35/100... Etape: 6490... Loss: 2.1918... Val Loss: 2.2131\n",
            "Epoch: 35/100... Etape: 6500... Loss: 2.2250... Val Loss: 2.2161\n",
            "Epoch: 35/100... Etape: 6510... Loss: 2.2196... Val Loss: 2.2153\n",
            "Epoch: 36/100... Etape: 6520... Loss: 2.2388... Val Loss: 2.2216\n",
            "Epoch: 36/100... Etape: 6530... Loss: 2.1821... Val Loss: 2.2093\n",
            "Epoch: 36/100... Etape: 6540... Loss: 2.1943... Val Loss: 2.2173\n",
            "Epoch: 36/100... Etape: 6550... Loss: 2.1580... Val Loss: 2.2107\n",
            "Epoch: 36/100... Etape: 6560... Loss: 2.2136... Val Loss: 2.2199\n",
            "Epoch: 36/100... Etape: 6570... Loss: 2.1831... Val Loss: 2.2154\n",
            "Epoch: 36/100... Etape: 6580... Loss: 2.2310... Val Loss: 2.2133\n",
            "Epoch: 36/100... Etape: 6590... Loss: 2.1572... Val Loss: 2.2111\n",
            "Epoch: 36/100... Etape: 6600... Loss: 2.2014... Val Loss: 2.2125\n",
            "Epoch: 36/100... Etape: 6610... Loss: 2.2314... Val Loss: 2.2104\n",
            "Epoch: 36/100... Etape: 6620... Loss: 2.2413... Val Loss: 2.2113\n",
            "Epoch: 36/100... Etape: 6630... Loss: 2.1948... Val Loss: 2.2146\n",
            "Epoch: 36/100... Etape: 6640... Loss: 2.2281... Val Loss: 2.2111\n",
            "Epoch: 36/100... Etape: 6650... Loss: 2.2537... Val Loss: 2.2111\n",
            "Epoch: 36/100... Etape: 6660... Loss: 2.2132... Val Loss: 2.2117\n",
            "Epoch: 36/100... Etape: 6670... Loss: 2.2027... Val Loss: 2.2102\n",
            "Epoch: 36/100... Etape: 6680... Loss: 2.2502... Val Loss: 2.2124\n",
            "Epoch: 36/100... Etape: 6690... Loss: 2.1799... Val Loss: 2.2077\n",
            "Epoch: 37/100... Etape: 6700... Loss: 2.2231... Val Loss: 2.2163\n",
            "Epoch: 37/100... Etape: 6710... Loss: 2.1906... Val Loss: 2.2112\n",
            "Epoch: 37/100... Etape: 6720... Loss: 2.1768... Val Loss: 2.2076\n",
            "Epoch: 37/100... Etape: 6730... Loss: 2.2118... Val Loss: 2.2121\n",
            "Epoch: 37/100... Etape: 6740... Loss: 2.1750... Val Loss: 2.2096\n",
            "Epoch: 37/100... Etape: 6750... Loss: 2.1891... Val Loss: 2.2155\n",
            "Epoch: 37/100... Etape: 6760... Loss: 2.1936... Val Loss: 2.2124\n",
            "Epoch: 37/100... Etape: 6770... Loss: 2.2337... Val Loss: 2.2069\n",
            "Epoch: 37/100... Etape: 6780... Loss: 2.2386... Val Loss: 2.2081\n",
            "Epoch: 37/100... Etape: 6790... Loss: 2.2644... Val Loss: 2.2058\n",
            "Epoch: 37/100... Etape: 6800... Loss: 2.2337... Val Loss: 2.2068\n",
            "Epoch: 37/100... Etape: 6810... Loss: 2.2249... Val Loss: 2.2102\n",
            "Epoch: 37/100... Etape: 6820... Loss: 2.2325... Val Loss: 2.2075\n",
            "Epoch: 37/100... Etape: 6830... Loss: 2.2019... Val Loss: 2.2088\n",
            "Epoch: 37/100... Etape: 6840... Loss: 2.2430... Val Loss: 2.2042\n",
            "Epoch: 37/100... Etape: 6850... Loss: 2.2217... Val Loss: 2.2087\n",
            "Epoch: 37/100... Etape: 6860... Loss: 2.1798... Val Loss: 2.2034\n",
            "Epoch: 37/100... Etape: 6870... Loss: 2.1510... Val Loss: 2.2067\n",
            "Epoch: 37/100... Etape: 6880... Loss: 2.2879... Val Loss: 2.2072\n",
            "Epoch: 38/100... Etape: 6890... Loss: 2.2130... Val Loss: 2.2114\n",
            "Epoch: 38/100... Etape: 6900... Loss: 2.1995... Val Loss: 2.2050\n",
            "Epoch: 38/100... Etape: 6910... Loss: 2.2150... Val Loss: 2.2067\n",
            "Epoch: 38/100... Etape: 6920... Loss: 2.1870... Val Loss: 2.2055\n",
            "Epoch: 38/100... Etape: 6930... Loss: 2.1985... Val Loss: 2.2078\n",
            "Epoch: 38/100... Etape: 6940... Loss: 2.2142... Val Loss: 2.2081\n",
            "Epoch: 38/100... Etape: 6950... Loss: 2.1766... Val Loss: 2.2036\n",
            "Epoch: 38/100... Etape: 6960... Loss: 2.1844... Val Loss: 2.2016\n",
            "Epoch: 38/100... Etape: 6970... Loss: 2.1999... Val Loss: 2.2025\n",
            "Epoch: 38/100... Etape: 6980... Loss: 2.2111... Val Loss: 2.2030\n",
            "Epoch: 38/100... Etape: 6990... Loss: 2.2775... Val Loss: 2.2026\n",
            "Epoch: 38/100... Etape: 7000... Loss: 2.2432... Val Loss: 2.2069\n",
            "Epoch: 38/100... Etape: 7010... Loss: 2.2046... Val Loss: 2.2052\n",
            "Epoch: 38/100... Etape: 7020... Loss: 2.2173... Val Loss: 2.2045\n",
            "Epoch: 38/100... Etape: 7030... Loss: 2.2167... Val Loss: 2.2017\n",
            "Epoch: 38/100... Etape: 7040... Loss: 2.2072... Val Loss: 2.2007\n",
            "Epoch: 38/100... Etape: 7050... Loss: 2.1808... Val Loss: 2.2028\n",
            "Epoch: 38/100... Etape: 7060... Loss: 2.2139... Val Loss: 2.2013\n",
            "Epoch: 39/100... Etape: 7070... Loss: 2.2063... Val Loss: 2.2065\n",
            "Epoch: 39/100... Etape: 7080... Loss: 2.1847... Val Loss: 2.2051\n",
            "Epoch: 39/100... Etape: 7090... Loss: 2.1694... Val Loss: 2.2024\n",
            "Epoch: 39/100... Etape: 7100... Loss: 2.2700... Val Loss: 2.2060\n",
            "Epoch: 39/100... Etape: 7110... Loss: 2.1776... Val Loss: 2.1999\n",
            "Epoch: 39/100... Etape: 7120... Loss: 2.2411... Val Loss: 2.2067\n",
            "Epoch: 39/100... Etape: 7130... Loss: 2.2190... Val Loss: 2.2020\n",
            "Epoch: 39/100... Etape: 7140... Loss: 2.1992... Val Loss: 2.1981\n",
            "Epoch: 39/100... Etape: 7150... Loss: 2.1751... Val Loss: 2.1991\n",
            "Epoch: 39/100... Etape: 7160... Loss: 2.2163... Val Loss: 2.1978\n",
            "Epoch: 39/100... Etape: 7170... Loss: 2.2621... Val Loss: 2.1994\n",
            "Epoch: 39/100... Etape: 7180... Loss: 2.1962... Val Loss: 2.2012\n",
            "Epoch: 39/100... Etape: 7190... Loss: 2.2013... Val Loss: 2.2027\n",
            "Epoch: 39/100... Etape: 7200... Loss: 2.2329... Val Loss: 2.2016\n",
            "Epoch: 39/100... Etape: 7210... Loss: 2.2504... Val Loss: 2.2005\n",
            "Epoch: 39/100... Etape: 7220... Loss: 2.1931... Val Loss: 2.2019\n",
            "Epoch: 39/100... Etape: 7230... Loss: 2.1973... Val Loss: 2.1969\n",
            "Epoch: 39/100... Etape: 7240... Loss: 2.1960... Val Loss: 2.2010\n",
            "Epoch: 39/100... Etape: 7250... Loss: 2.1962... Val Loss: 2.1985\n",
            "Epoch: 40/100... Etape: 7260... Loss: 2.2149... Val Loss: 2.2063\n",
            "Epoch: 40/100... Etape: 7270... Loss: 2.1703... Val Loss: 2.1986\n",
            "Epoch: 40/100... Etape: 7280... Loss: 2.2206... Val Loss: 2.1971\n",
            "Epoch: 40/100... Etape: 7290... Loss: 2.2040... Val Loss: 2.1995\n",
            "Epoch: 40/100... Etape: 7300... Loss: 2.1371... Val Loss: 2.1979\n",
            "Epoch: 40/100... Etape: 7310... Loss: 2.1687... Val Loss: 2.2034\n",
            "Epoch: 40/100... Etape: 7320... Loss: 2.1701... Val Loss: 2.1960\n",
            "Epoch: 40/100... Etape: 7330... Loss: 2.1791... Val Loss: 2.1970\n",
            "Epoch: 40/100... Etape: 7340... Loss: 2.1314... Val Loss: 2.1963\n",
            "Epoch: 40/100... Etape: 7350... Loss: 2.1905... Val Loss: 2.1945\n",
            "Epoch: 40/100... Etape: 7360... Loss: 2.1947... Val Loss: 2.1960\n",
            "Epoch: 40/100... Etape: 7370... Loss: 2.2171... Val Loss: 2.2010\n",
            "Epoch: 40/100... Etape: 7380... Loss: 2.1767... Val Loss: 2.1980\n",
            "Epoch: 40/100... Etape: 7390... Loss: 2.2034... Val Loss: 2.1975\n",
            "Epoch: 40/100... Etape: 7400... Loss: 2.1855... Val Loss: 2.1937\n",
            "Epoch: 40/100... Etape: 7410... Loss: 2.1962... Val Loss: 2.1987\n",
            "Epoch: 40/100... Etape: 7420... Loss: 2.1975... Val Loss: 2.1925\n",
            "Epoch: 40/100... Etape: 7430... Loss: 2.1989... Val Loss: 2.1960\n",
            "Epoch: 40/100... Etape: 7440... Loss: 2.1858... Val Loss: 2.1959\n",
            "Epoch: 41/100... Etape: 7450... Loss: 2.2170... Val Loss: 2.2026\n",
            "Epoch: 41/100... Etape: 7460... Loss: 2.1606... Val Loss: 2.1937\n",
            "Epoch: 41/100... Etape: 7470... Loss: 2.1797... Val Loss: 2.1969\n",
            "Epoch: 41/100... Etape: 7480... Loss: 2.1643... Val Loss: 2.1937\n",
            "Epoch: 41/100... Etape: 7490... Loss: 2.2007... Val Loss: 2.1969\n",
            "Epoch: 41/100... Etape: 7500... Loss: 2.1771... Val Loss: 2.1968\n",
            "Epoch: 41/100... Etape: 7510... Loss: 2.2239... Val Loss: 2.1923\n",
            "Epoch: 41/100... Etape: 7520... Loss: 2.1390... Val Loss: 2.1929\n",
            "Epoch: 41/100... Etape: 7530... Loss: 2.1851... Val Loss: 2.1914\n",
            "Epoch: 41/100... Etape: 7540... Loss: 2.2055... Val Loss: 2.1912\n",
            "Epoch: 41/100... Etape: 7550... Loss: 2.2071... Val Loss: 2.1932\n",
            "Epoch: 41/100... Etape: 7560... Loss: 2.1655... Val Loss: 2.1952\n",
            "Epoch: 41/100... Etape: 7570... Loss: 2.1981... Val Loss: 2.1930\n",
            "Epoch: 41/100... Etape: 7580... Loss: 2.2164... Val Loss: 2.1928\n",
            "Epoch: 41/100... Etape: 7590... Loss: 2.2017... Val Loss: 2.1912\n",
            "Epoch: 41/100... Etape: 7600... Loss: 2.1726... Val Loss: 2.1917\n",
            "Epoch: 41/100... Etape: 7610... Loss: 2.2174... Val Loss: 2.1899\n",
            "Epoch: 41/100... Etape: 7620... Loss: 2.1607... Val Loss: 2.1906\n",
            "Epoch: 42/100... Etape: 7630... Loss: 2.1891... Val Loss: 2.1966\n",
            "Epoch: 42/100... Etape: 7640... Loss: 2.1739... Val Loss: 2.1928\n",
            "Epoch: 42/100... Etape: 7650... Loss: 2.1602... Val Loss: 2.1889\n",
            "Epoch: 42/100... Etape: 7660... Loss: 2.1945... Val Loss: 2.1953\n",
            "Epoch: 42/100... Etape: 7670... Loss: 2.1420... Val Loss: 2.1915\n",
            "Epoch: 42/100... Etape: 7680... Loss: 2.1769... Val Loss: 2.1979\n",
            "Epoch: 42/100... Etape: 7690... Loss: 2.1689... Val Loss: 2.1896\n",
            "Epoch: 42/100... Etape: 7700... Loss: 2.2052... Val Loss: 2.1894\n",
            "Epoch: 42/100... Etape: 7710... Loss: 2.2194... Val Loss: 2.1886\n",
            "Epoch: 42/100... Etape: 7720... Loss: 2.2435... Val Loss: 2.1890\n",
            "Epoch: 42/100... Etape: 7730... Loss: 2.2115... Val Loss: 2.1881\n",
            "Epoch: 42/100... Etape: 7740... Loss: 2.1864... Val Loss: 2.1920\n",
            "Epoch: 42/100... Etape: 7750... Loss: 2.2293... Val Loss: 2.1912\n",
            "Epoch: 42/100... Etape: 7760... Loss: 2.1892... Val Loss: 2.1898\n",
            "Epoch: 42/100... Etape: 7770... Loss: 2.2115... Val Loss: 2.1874\n",
            "Epoch: 42/100... Etape: 7780... Loss: 2.1914... Val Loss: 2.1901\n",
            "Epoch: 42/100... Etape: 7790... Loss: 2.1882... Val Loss: 2.1858\n",
            "Epoch: 42/100... Etape: 7800... Loss: 2.1317... Val Loss: 2.1897\n",
            "Epoch: 42/100... Etape: 7810... Loss: 2.2529... Val Loss: 2.1877\n",
            "Epoch: 43/100... Etape: 7820... Loss: 2.1903... Val Loss: 2.1955\n",
            "Epoch: 43/100... Etape: 7830... Loss: 2.1716... Val Loss: 2.1876\n",
            "Epoch: 43/100... Etape: 7840... Loss: 2.2216... Val Loss: 2.1886\n",
            "Epoch: 43/100... Etape: 7850... Loss: 2.1740... Val Loss: 2.1900\n",
            "Epoch: 43/100... Etape: 7860... Loss: 2.1818... Val Loss: 2.1898\n",
            "Epoch: 43/100... Etape: 7870... Loss: 2.1973... Val Loss: 2.1911\n",
            "Epoch: 43/100... Etape: 7880... Loss: 2.1538... Val Loss: 2.1844\n",
            "Epoch: 43/100... Etape: 7890... Loss: 2.1601... Val Loss: 2.1853\n",
            "Epoch: 43/100... Etape: 7900... Loss: 2.1826... Val Loss: 2.1833\n",
            "Epoch: 43/100... Etape: 7910... Loss: 2.1629... Val Loss: 2.1846\n",
            "Epoch: 43/100... Etape: 7920... Loss: 2.2448... Val Loss: 2.1874\n",
            "Epoch: 43/100... Etape: 7930... Loss: 2.2324... Val Loss: 2.1881\n",
            "Epoch: 43/100... Etape: 7940... Loss: 2.1894... Val Loss: 2.1865\n",
            "Epoch: 43/100... Etape: 7950... Loss: 2.2002... Val Loss: 2.1859\n",
            "Epoch: 43/100... Etape: 7960... Loss: 2.2041... Val Loss: 2.1834\n",
            "Epoch: 43/100... Etape: 7970... Loss: 2.1940... Val Loss: 2.1864\n",
            "Epoch: 43/100... Etape: 7980... Loss: 2.1784... Val Loss: 2.1841\n",
            "Epoch: 43/100... Etape: 7990... Loss: 2.1891... Val Loss: 2.1846\n",
            "Epoch: 44/100... Etape: 8000... Loss: 2.1833... Val Loss: 2.1880\n",
            "Epoch: 44/100... Etape: 8010... Loss: 2.1623... Val Loss: 2.1897\n",
            "Epoch: 44/100... Etape: 8020... Loss: 2.1481... Val Loss: 2.1829\n",
            "Epoch: 44/100... Etape: 8030... Loss: 2.2339... Val Loss: 2.1890\n",
            "Epoch: 44/100... Etape: 8040... Loss: 2.1423... Val Loss: 2.1859\n",
            "Epoch: 44/100... Etape: 8050... Loss: 2.2082... Val Loss: 2.1892\n",
            "Epoch: 44/100... Etape: 8060... Loss: 2.2094... Val Loss: 2.1835\n",
            "Epoch: 44/100... Etape: 8070... Loss: 2.1815... Val Loss: 2.1821\n",
            "Epoch: 44/100... Etape: 8080... Loss: 2.1665... Val Loss: 2.1827\n",
            "Epoch: 44/100... Etape: 8090... Loss: 2.1998... Val Loss: 2.1804\n",
            "Epoch: 44/100... Etape: 8100... Loss: 2.2292... Val Loss: 2.1818\n",
            "Epoch: 44/100... Etape: 8110... Loss: 2.1411... Val Loss: 2.1850\n",
            "Epoch: 44/100... Etape: 8120... Loss: 2.1758... Val Loss: 2.1858\n",
            "Epoch: 44/100... Etape: 8130... Loss: 2.2076... Val Loss: 2.1835\n",
            "Epoch: 44/100... Etape: 8140... Loss: 2.2163... Val Loss: 2.1808\n",
            "Epoch: 44/100... Etape: 8150... Loss: 2.1946... Val Loss: 2.1828\n",
            "Epoch: 44/100... Etape: 8160... Loss: 2.1807... Val Loss: 2.1791\n",
            "Epoch: 44/100... Etape: 8170... Loss: 2.1648... Val Loss: 2.1824\n",
            "Epoch: 44/100... Etape: 8180... Loss: 2.1777... Val Loss: 2.1804\n",
            "Epoch: 45/100... Etape: 8190... Loss: 2.2072... Val Loss: 2.1883\n",
            "Epoch: 45/100... Etape: 8200... Loss: 2.1478... Val Loss: 2.1836\n",
            "Epoch: 45/100... Etape: 8210... Loss: 2.1808... Val Loss: 2.1792\n",
            "Epoch: 45/100... Etape: 8220... Loss: 2.1914... Val Loss: 2.1851\n",
            "Epoch: 45/100... Etape: 8230... Loss: 2.0976... Val Loss: 2.1823\n",
            "Epoch: 45/100... Etape: 8240... Loss: 2.1472... Val Loss: 2.1858\n",
            "Epoch: 45/100... Etape: 8250... Loss: 2.1732... Val Loss: 2.1812\n",
            "Epoch: 45/100... Etape: 8260... Loss: 2.1836... Val Loss: 2.1773\n",
            "Epoch: 45/100... Etape: 8270... Loss: 2.1390... Val Loss: 2.1802\n",
            "Epoch: 45/100... Etape: 8280... Loss: 2.1580... Val Loss: 2.1786\n",
            "Epoch: 45/100... Etape: 8290... Loss: 2.1721... Val Loss: 2.1783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Darwish_model.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(model_name, f)"
      ],
      "metadata": {
        "id": "UUhZUL6jmBFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with open('Darwish_model.net', 'rb') as f:\n",
        "    if train_on_gpu:\n",
        "            checkpoint = torch.load(f)\n",
        "    else:        \n",
        "        checkpoint = torch.load(f,map_location=torch.device('cpu'))\n",
        "\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])\n",
        "'''"
      ],
      "metadata": {
        "id": "imUVR2uqkEk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        \n",
        "        x = np.array([[net.char_to_int[char]]])\n",
        "        x = one_hot(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        \n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # Calculer les probabilités du prochain caractère\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # passer au cpu\n",
        "        \n",
        "        # Garder les caractères avec les plus grandes probabiités\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # Choisir le prochain caractère à l'aide d'une fonction aléatoire\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # Retourner le code du caractère et le convertir en caractère  \n",
        "        return net.int_to_char[char], h"
      ],
      "metadata": {
        "id": "kdVm8Fhlkyup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # mode d'évaluation\n",
        "    \n",
        "    # D'abord, traiter le premier \n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Passer le caractère précédent pour avoir le prochaine\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "bJVIpSpPk1wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 300, prime='السلام', top_k=5))"
      ],
      "metadata": {
        "id": "JlRfZ4Xfk3l6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}